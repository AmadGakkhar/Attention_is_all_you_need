import torch
import torch.nn as nn
from typing import Any
from torch.utils.data import Dataset


class BilingualDataset(Dataset):
    def __init__(self, ds, tokenizer_src, tokenizer_trg, src_lang, trg_lang, seq_len):

        super().__init__()

        self.ds = ds
        self.tokenizer_src = tokenizer_src
        self.tokenizer_trg = tokenizer_trg
        self.src_lang = src_lang
        self.trg_lang = trg_lang
        self.seq_len = seq_len

        self.sos_token = torch.Tensor(
            [self.tokenizer_src.token_to_id(["[SOS]"])], dtype=torch.int64
        )

        self.eos_token = torch.Tensor(
            [self.tokenizer_src.token_to_id(["[EOS]"])], dtype=torch.int64
        )

        self.pad_token = torch.Tensor(
            [self.tokenizer_src.token_to_id(["[PAD]"])], dtype=torch.int64
        )

    def __len__(self):
        return len(self.ds)

    def __getitem__(self, index) -> Any:
        """
        Returns a dictionary containing the encoder input, decoder input, encoder mask, decoder mask, and label as tensors.
        The encoder input and decoder input are padded to have the same length as the sequence length.
        The encoder mask and decoder mask are generated by comparing the input tensors with the pad token.
        The label is the same as the decoder input but with the EOS token appended at the end.
        The src_text and trg_text are the original text for the source and target languages, respectively.

        Args:
            index (int): The index of the sentence pair in the dataset.

        Returns:
            A dictionary containing the following keys:
                - encoder_input (torch.Tensor): The input tensor for the encoder model. (seq_len)
                - decoder_input (torch.Tensor): The input tensor for the decoder model. (seq_len)
                - encoder_mask (torch.Tensor): The mask tensor for the encoder model. (1,1,seq_len)
                - decoder_mask (torch.Tensor): The mask tensor for the decoder model. (1,seq_len)
                - label (torch.Tensor): The label tensor for the decoder model. (seq_len)
                - src_text (str): The original text for the source language.
                - trg_text (str): The original text for the target language.
        """
        src_target_pair = self.ds[index]

        src_text = src_target_pair["translation"][self.src_lang]
        trg_text = src_target_pair["translation"][self.trg_lang]

        enc_input_tokens = self.tokenizer_src.encode(src_text).ids
        dec_input_tokens = self.tokenizer_trg.encode(trg_text).ids

        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2
        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1

        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:
            raise ValueError("Sentence Too Long")

        encoder_input = torch.cat(
            [
                self.sos_token,
                torch.Tensor(enc_input_tokens),
                torch.Tensor(
                    [self.pad_token] * enc_num_padding_tokens, dtype=torch.int64
                ),
            ]
        )
        decoder_input = torch.cat(
            [
                self.sos_token,
                torch.Tensor(dec_input_tokens),
                torch.Tensor(
                    [self.pad_token] * dec_num_padding_tokens, dtype=torch.int64
                ),
            ]
        )

        label = torch.cat(
            [
                torch.Tensor(dec_input_tokens),
                self.eos_token,
                torch.Tensor(
                    [self.pad_token] * dec_num_padding_tokens, dtype=torch.int64
                ),
            ]
        )
        assert encoder_input.size(0) == self.seq_len
        assert decoder_input.size(0) == self.seq_len
        assert label.size(0) == self.seq_len

        return {
            "encoder_input": encoder_input,
            "decoder_input": decoder_input,
            "encoder_mask": (encoder_input != self.pad_token)
            .unsqueeze(0)
            .unsqueeze(0)
            .int(),  # (1,1,seq_len)
            "decoder_mask": (decoder_input != self.pad_token)
            .unsqueeze(0)
            .unsqueeze(0)
            .int()
            & causal_mask(decoder_input.size(0)),  # (1,seq_len) & (1,1,seq_len
            "label": label,  # (seq_len)
            "src_text": src_text,
            "trg_text": trg_text,
        }


def causal_mask(size):
    """
    Generates a causal mask of size (1, seq_len, seq_len). The diagonal and lower
    triangular entries are set to 1, and the upper triangular entries are set to 0.
    The mask is then inverted and returned as a tensor of type torch.int.
    """
    mask = torch.triu(torch.ones(1, size, size), diaginal=1).type(torch.int)
    return mask == 0
